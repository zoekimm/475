{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Final Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrvhhySvGanD"
      },
      "source": [
        "input = [] #English\n",
        "output = [] #Russian\n",
        "\n",
        "with open('./english-russian.txt', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        sentences = line.split('\\t')[:2]\n",
        "        input.append(sentences[1])\n",
        "        output.append(sentences[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTK_O7yiodJI"
      },
      "source": [
        "import string\n",
        "\n",
        "#remove punctuations\n",
        "input = [''.join(word for word in sentence if word not in string.punctuation) for sentence in input]\n",
        "output = [''.join(word for word in sentence if word not in string.punctuation) for sentence in output]\n",
        "\n",
        "#need to add SOS EOS token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnsQHA6hli5W"
      },
      "source": [
        "from collections import Counter\n",
        "from itertools import chain\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "class Vocab:\n",
        "  def __init__(self, li): \n",
        "      #li = list of sentencs\n",
        "        #self.language = language #eng OR rus\n",
        "        self.li = li\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.listOfVocab = self.get_vocab()\n",
        "        self.word2count = self.get_word2count()\n",
        "        self.word2index = self.get_word2index()\n",
        "        self.max_length = max([len(sentence.split()) for sentence in self.li])\n",
        "        self.encoded = self.get_encoded()\n",
        "\n",
        "  def get_vocab(self): \n",
        "    temp = []\n",
        "    for i in self.li:\n",
        "      temp.append(i.split())\n",
        "    return list(chain(*temp)) #list of vocab\n",
        "\n",
        "  def get_word2count(self):\n",
        "    return Counter(self.listOfVocab)\n",
        "\n",
        "  def get_word2index(self):\n",
        "    self.tokenizer.fit_on_texts(self.li)\n",
        "    return self.tokenizer.word_index\n",
        "\n",
        "  def get_encoded(self):\n",
        "    sequence = self.tokenizer.texts_to_sequences(self.li)\n",
        "    sequence = pad_sequences(sequence, self.max_length, padding='post')\n",
        "    return sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecBku5ocOzv2"
      },
      "source": [
        "eng_vocab = Vocab(output)\n",
        "rus_vocab = Vocab(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOCHBa3gGgga"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = input, output\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCNXhR4wIrEz"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module): # shape (batch_size, maxlength)\n",
        "    def __init__(self, input_size, hidden_size, embedding_size, num_layers)):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, 0.5)\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, (h_n, c_n) = self.rnn(x)\n",
        "        return output, (h_n, c_n)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, 0.5)\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "class Seq2SeqModel(nn.Module):\n",
        "    def __init__(self, encoder, decoder, hidden_size, num_class):\n",
        "        super(Seq2SeqModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.fc = nn.Linear(hidden_size, num_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0cjOaR1ExEF"
      },
      "source": [
        "n_decoder_tokens = len(rus_vocab.word2count)\n",
        "\n",
        "def batch_generator(X, y, batch_size):\n",
        "  while True:\n",
        "    for i in range(len(X)//batch_size):\n",
        "      encoder_input = np.zeros(batch_size, eng_vocab.max_length)\n",
        "      decoder_input = np.zeros(batch_size, rus_vocab.max_length)\n",
        "      target = np.zeros(batch_size, rus_vocab.max_length, n_decoder_tokens)\n",
        "      for j in range(batch_size):\n",
        "        index = i * batch_size + j\n",
        "        if index >= len(X):\n",
        "          break\n",
        "        input_text = X[index]\n",
        "        target_text = y[index]\n",
        "        for k, word in enumerate(input_text.split()):\n",
        "          encoder_input[j, k] = eng_vocab.word2index[word]\n",
        "        for k, word in enumerate(target_text.split()):\n",
        "          if k < len(target_text.split()) - 1:\n",
        "            decoder_input[j, k] = rus_vocab.word2index[word]\n",
        "          if k > 0:\n",
        "            target[j, k-1, rus_vocab.word2index[word]] = 1\n",
        "\n",
        "      yield ([encoder_input, decoder_input], target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCBfGRJLKVe-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}