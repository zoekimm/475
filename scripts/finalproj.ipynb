{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Final Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrvhhySvGanD"
      },
      "source": [
        "input = [] #English\n",
        "output = [] #Russian\n",
        "\n",
        "with open('./english-russian.txt', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        sentences = line.split('\\t')[:2]\n",
        "        input.append(sentences[0])\n",
        "        output.append(sentences[1])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTK_O7yiodJI"
      },
      "source": [
        "import string\n",
        "\n",
        "#remove punctuations\n",
        "input = [''.join(word for word in sentence if word not in string.punctuation) for sentence in input]\n",
        "output = [''.join(word for word in sentence if word not in string.punctuation) for sentence in output]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnsQHA6hli5W"
      },
      "source": [
        "from collections import Counter\n",
        "from itertools import chain\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "class Vocab:\n",
        "  def __init__(self, li): \n",
        "      #li = list of sentencs\n",
        "        #self.language = language #eng OR rus\n",
        "        self.li = li\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.listOfVocab = self.get_vocab()\n",
        "        self.word2count = self.get_word2count()\n",
        "        self.word2index = self.get_word2index()\n",
        "        self.max_length = max([len(sentence) for sentence in self.li])\n",
        "        self.encoded = self.get_encoded()\n",
        "\n",
        "  def get_vocab(self): \n",
        "    temp = []\n",
        "    for i in self.li:\n",
        "      temp.append(i.split())\n",
        "    return list(chain(*temp)) #list of vocab\n",
        "\n",
        "  def get_word2count(self):\n",
        "    return Counter(self.listOfVocab)\n",
        "\n",
        "  def get_word2index(self):\n",
        "    self.tokenizer.fit_on_texts(self.li)\n",
        "    return self.tokenizer.word_index\n",
        "\n",
        "  def get_encoded(self):\n",
        "    sequence = self.tokenizer.texts_to_sequences(self.li)\n",
        "    sequence = pad_sequences(sequence, self.max_length, padding='post')\n",
        "    return sequence"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecBku5ocOzv2"
      },
      "source": [
        "eng_vocab = Vocab(input)\n",
        "rus_vocab = Vocab(output)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1_VIZxIqznM"
      },
      "source": [
        ""
      ]
    }
  ]
}
