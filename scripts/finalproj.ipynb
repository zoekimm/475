{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Final Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install unzip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0dbm5c9Z2qg",
        "outputId": "66ec911d-4d50-481f-dd6e-9111a48a0884"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-21ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://github.com/zoekimm/475/blob/main/scripts/english-russian.txt.zip?raw=true\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bsqt-m-rZ6Z7",
        "outputId": "2ee42c12-5ada-4902-f93c-99e1a6cb805f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-10 11:09:57--  https://github.com/zoekimm/475/blob/main/scripts/english-russian.txt.zip?raw=true\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/zoekimm/475/raw/main/scripts/english-russian.txt.zip [following]\n",
            "--2021-12-10 11:09:57--  https://github.com/zoekimm/475/raw/main/scripts/english-russian.txt.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/zoekimm/475/main/scripts/english-russian.txt.zip [following]\n",
            "--2021-12-10 11:09:57--  https://raw.githubusercontent.com/zoekimm/475/main/scripts/english-russian.txt.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12314874 (12M) [application/zip]\n",
            "Saving to: ‘english-russian.txt.zip?raw=true.1’\n",
            "\n",
            "english-russian.txt 100%[===================>]  11.74M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-12-10 11:09:58 (117 MB/s) - ‘english-russian.txt.zip?raw=true.1’ saved [12314874/12314874]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/english-russian.txt.zip?raw=true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UDr6-XccwBH",
        "outputId": "3dc84c9c-57ce-485b-954e-b26501847de8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/english-russian.txt.zip?raw=true\n",
            "replace english-russian.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: english-russian.txt     \n",
            "replace __MACOSX/._english-russian.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/._english-russian.txt  \n"
          ]
        }
      ]
    },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrvhhySvGanD"
      },
      "source": [
        "input = [] #English\n",
        "output = [] #Russian\n",
        "\n",
        "with open('./english-russian.txt', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        sentences = line.split('\\t')[:2]\n",
        "        input.append(sentences[1])\n",
        "        output.append(sentences[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTK_O7yiodJI"
      },
      "source": [
        "import string\n",
        "\n",
        "#remove punctuations\n",
        "input = [''.join(word for word in sentence if word not in string.punctuation) for sentence in input]\n",
        "output = [''.join(word for word in sentence if word not in string.punctuation) for sentence in output]\n",
        "\n",
        "#need to add SOS EOS token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnsQHA6hli5W"
      },
      "source": [
        "from collections import Counter\n",
        "from itertools import chain\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "class Vocab:\n",
        "  def __init__(self, li): \n",
        "      #li = list of sentencs\n",
        "        #self.language = language #eng OR rus\n",
        "        self.li = li\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.listOfVocab = self.get_vocab()\n",
        "        self.word2count = self.get_word2count()\n",
        "        self.word2index = self.get_word2index()\n",
        "        self.max_length = max([len(sentence.split()) for sentence in self.li])\n",
        "        self.encoded = self.get_encoded()\n",
        "\n",
        "  def get_vocab(self): \n",
        "    temp = []\n",
        "    for i in self.li:\n",
        "      temp.append(i.split())\n",
        "    return list(chain(*temp)) #list of vocab\n",
        "\n",
        "  def get_word2count(self):\n",
        "    return Counter(self.listOfVocab)\n",
        "\n",
        "  def get_word2index(self):\n",
        "    self.tokenizer.fit_on_texts(self.li)\n",
        "    return self.tokenizer.word_index\n",
        "\n",
        "  def get_encoded(self):\n",
        "    sequence = self.tokenizer.texts_to_sequences(self.li)\n",
        "    sequence = pad_sequences(sequence, self.max_length, padding='post')\n",
        "    return sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecBku5ocOzv2"
      },
      "source": [
        "eng_vocab = Vocab(output)\n",
        "rus_vocab = Vocab(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOCHBa3gGgga"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = input, output\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCNXhR4wIrEz"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module): # shape (batch_size, maxlength)\n",
        "    def __init__(self, input_size, hidden_size, embedding_size, num_layers)):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, 0.5)\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, (h_n, c_n) = self.rnn(x)\n",
        "        return output, (h_n, c_n)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, 0.5)\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "class Seq2SeqModel(nn.Module):\n",
        "    def __init__(self, encoder, decoder, hidden_size, num_class):\n",
        "        super(Seq2SeqModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.fc = nn.Linear(hidden_size, num_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0cjOaR1ExEF"
      },
      "source": [
        "n_decoder_tokens = len(rus_vocab.word2count)\n",
        "\n",
        "def batch_generator(X, y, batch_size):\n",
        "  while True:\n",
        "    for i in range(len(X)//batch_size):\n",
        "      encoder_input = np.zeros(batch_size, eng_vocab.max_length)\n",
        "      decoder_input = np.zeros(batch_size, rus_vocab.max_length)\n",
        "      target = np.zeros(batch_size, rus_vocab.max_length, n_decoder_tokens)\n",
        "      for j in range(batch_size):\n",
        "        index = i * batch_size + j\n",
        "        if index >= len(X):\n",
        "          break\n",
        "        input_text = X[index]\n",
        "        target_text = y[index]\n",
        "        for k, word in enumerate(input_text.split()):\n",
        "          encoder_input[j, k] = eng_vocab.word2index[word]\n",
        "        for k, word in enumerate(target_text.split()):\n",
        "          if k < len(target_text.split()) - 1:\n",
        "            decoder_input[j, k] = rus_vocab.word2index[word]\n",
        "          if k > 0:\n",
        "            target[j, k-1, rus_vocab.word2index[word]] = 1\n",
        "\n",
        "      yield ([encoder_input, decoder_input], target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCBfGRJLKVe-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
